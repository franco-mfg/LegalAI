# version: '3.1'
services:
  ollama:
    container_name: ollama-llm-ai
    image: ollama/ollama
    healthcheck:
      test: ollama --version || exit 1
      interval: 60s
    ports:
      - "11434:11434"
    volumes:
      - ./dati_ollama:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=30m
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "6.0"
    #       memory: "16G"
    
# #######################
  legalai:
    container_name: myDevTest
    build: .
    # stop_signal: SIGINT
    image: deb-legalai-v0
    depends_on:
      - ollama
    ports:
      - "5000:5000"
    volumes:
      - ./dbdata:/legalai/db
      - ./python_venv:/legalai/.venv
    environment:
      - FLASK_RUN_PORT=8000
      # FLASK_HOST='0.0.0.0'
      - OLLAMA_BASE_URL=http://ollama:11434/
      - LLM_MODEL=qwen2:1.5b
      # LLM_MODEL=llama3.1
      - EMBED_MODEL=all-MiniLM-L6-v2
      # - EMBED_MODEL=BAAI/bge-base-en-v1.5
      - EURLEX_TABLE=test
      - EURLEX_NUM_REC=100
      - SLEEP=5
      - DEBUG=true
